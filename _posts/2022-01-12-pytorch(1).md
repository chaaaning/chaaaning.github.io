---
layout: post
title: "[Pytorch] Part1-4.반드시 알아야하는 파이토치 스킬"
date: 2022-01-12 23:11 +0900
categories : Study
tags: [pytorch]
toc:  true
math: true
---

1. **Tensor**
   - **Scalar**
   - **Vector**
   - **Matrix**
   - **Tensor**
2. **Autograd**

## Tensor

---

### Scalar

---

- 말 그대로 양이 존재하는 상수 값을 의미함.
- 사칙 연산 수행.
- 하나의 값을 표현할 때 1개의 수치로 표현.

> **scalar code cell**

```pyton
import torch

sc1 = torch.tensor([1.])
sc2 = torch.tensor([3.])
print(f"{sc1} ::: {sc2}")
```
```text
---output---
tensor([1.]) ::: tensor([3.])
```
> **단순한 사칙연산 / torch 내장 함수**

```python
# 단순 사칙연산
print(f"add_scalar : {sc1+sc2}")
print(f"sub_scalar : {sc1-sc2}")
print(f"mul_scalar : {sc1*sc2}")
print(f"div_scalar : {sc1/sc2}")

# torch 내장 함수
print(f"add_scalar {torch.add(sc1,sc2)}")
print(f"sub_scalar {torch.sub(sc1,sc2)}")
print(f"mul_scalar {torch.mul(sc1,sc2)}")
print(f"div_scalar {torch.div(sc1,sc2)}")
```
```text
--- output ---
add_scalar : tensor([4.])
sub_scalar : tensor([-2.])
mul_scalar : tensor([3.])
div_scalar : tensor([0.3333])

add_scalar tensor([4.])
sub_scalar tensor([-2.])
mul_scalar tensor([3.])
div_scalar tensor([0.3333])
```
<br><br>

### Vector

---

- 2개 이상 수치 표현
- 말 그대로 요소가 있는 벡터

> **vector code cell**

```python
v1, v2 = torch.tensor([1., 2., 3.]), torch.tensor([4., 5., 6.])
print(v1, v2)
```
```text
--- output ---
tensor([1., 2., 3.]) tensor([4., 5., 6.])
```
> **단순한 사칙연산 / torch 내장 함수**

```python
# 단순 사칙연산
print(f"add_scalar : {v1+v2}")
print(f"sub_scalar : {v1-v2}")
print(f"mul_scalar : {v1*v2}")
print(f"div_scalar : {v1/v2}")

# torch 내장 함수
print(f"add_scalar {torch.add(v1,v2)}")
print(f"sub_scalar {torch.sub(v1,v2)}")
print(f"mul_scalar {torch.mul(v1,v2)}")
print(f"div_scalar {torch.div(v1,v2)}")
print(f"dot_scalar {torch.dot(v1,v2)}")
```
```text
--- output ---
add_scalar : tensor([5., 7., 9.])
sub_scalar : tensor([-3., -3., -3.])
mul_scalar : tensor([ 4., 10., 18.])
div_scalar : tensor([0.2500, 0.4000, 0.5000])

add_scalar tensor([5., 7., 9.])
sub_scalar tensor([-3., -3., -3.])
mul_scalar tensor([ 4., 10., 18.])
div_scalar tensor([0.2500, 0.4000, 0.5000])
dot_scalar 32.0
```
<br><br>

### Matrix

---

> **matrix code cell**

```pyton
mat1, mat2 = torch.tensor(
    [[1., 2.], [3., 4.]]), torch.tensor([[5., 6.], [7., 8.]])
print(mat1, "\n", mat2)
```
```text
--- output ---
tensor([[1., 2.],
        [3., 4.]]) 
 tensor([[5., 6.],
        [7., 8.]])
```

> **단순한 사칙연산 / torch 내장 함수**

```python
# 단순 사칙연산
print(f"add_scalar : {mat1+mat2}")
print(f"sub_scalar : {mat1-mat2}")
print(f"mul_scalar : {mat1*mat2}")
print(f"div_scalar : {mat1/mat2}")

# torch 내장 함수
print(f"add_scalar {torch.add(mat1,mat2)}")
print(f"sub_scalar {torch.sub(mat1,mat2)}")
print(f"mul_scalar {torch.mul(mat1,mat2)}")
print(f"div_scalar {torch.div(mat1,mat2)}")
print(f"mat_mul_scalar {torch.matmul(mat1,mat2)}")
```
```text
--- output ---
add_scalar : tensor([[ 6.,  8.],
        [10., 12.]])
sub_scalar : tensor([[-4., -4.],
        [-4., -4.]])
mul_scalar : tensor([[ 5., 12.],
        [21., 32.]])
div_scalar : tensor([[0.2000, 0.3333],
        [0.4286, 0.5000]])

add_scalar tensor([[ 6.,  8.],
        [10., 12.]])
sub_scalar tensor([[-4., -4.],
        [-4., -4.]])
mul_scalar tensor([[ 5., 12.],
        [21., 32.]])
div_scalar tensor([[0.2000, 0.3333],
        [0.4286, 0.5000]])
mat_mul_scalar tensor([[19., 22.],
        [43., 50.]])
```
<br><br>

### Tensor

---

![스칼라 벡터 행렬 텐서 이미지](https://miro.medium.com/max/1400/1*pUr-9ctuGamgjSwoW_KU-A.png)


<span style="color:silver">*출처 : [Linear Algebra for Deep Learning](https://towardsdatascience.com/linear-algebra-for-deep-learning-506c19c0d6fa)* </span>


- 위와 같이 텐서는 2차원 이상의 배열이라 표현 할 수 있음
- 즉 사칙 연산이 위치에 맞게끔 이루어짐

> **tensor code cell**

```python
tensor1 = torch.tensor([[[1., 2.], [3., 4.]], [[5., 6.], [7., 8.]]])
print(tensor1)
print()
tensor2 = torch.tensor([[[9., 10.], [11., 12.]], [[13., 14.], [15., 16.]]])
print(tensor2)
```
```text
--- output ---
tensor([[[1., 2.],
         [3., 4.]],

        [[5., 6.],
         [7., 8.]]])

tensor([[[ 9., 10.],
         [11., 12.]],

        [[13., 14.],
         [15., 16.]]])
```

> **단순한 사칙연산 / torch 내장 함수**

```python
# 단순 사칙연산
print(tensor1+tensor2)  # add
print(tensor1-tensor2)  # sub
print(tensor1*tensor2)  # multiple
print(tensor1/tensor2)  # divide

# torch 내장 함수
print(torch.add(tensor1, tensor2))
print(torch.sub(tensor1, tensor2))
print(torch.mul(tensor1, tensor2))
print(torch.div(tensor1, tensor2))
print(torch.matmul(tensor1, tensor2))
```
```text
--- output ---
tensor([[[10., 12.],
         [14., 16.]],

        [[18., 20.],
         [22., 24.]]])
tensor([[[-8., -8.],
         [-8., -8.]],

        [[-8., -8.],
         [-8., -8.]]])
tensor([[[  9.,  20.],
         [ 33.,  48.]],

        [[ 65.,  84.],
         [105., 128.]]])
tensor([[[0.1111, 0.2000],
         [0.2727, 0.3333]],

        [[0.3846, 0.4286],
         [0.4667, 0.5000]]])

tensor([[[10., 12.],
         [14., 16.]],

        [[18., 20.],
         [22., 24.]]])
tensor([[[-8., -8.],
         [-8., -8.]],

        [[-8., -8.],
         [-8., -8.]]])
tensor([[[  9.,  20.],
         [ 33.,  48.]],

        [[ 65.,  84.],
         [105., 128.]]])
tensor([[[0.1111, 0.2000],
         [0.2727, 0.3333]],

        [[0.3846, 0.4286],
         [0.4667, 0.5000]]])
tensor([[[ 31.,  34.],
         [ 71.,  78.]],

        [[155., 166.],
         [211., 226.]]])
```
<br><br>

## Autograd

---

파이토치를 이용하여 코드를 작성할 때 Back Propagation을 이용해 파라미터를 업데이트하는 방법은 Autograd 방식으로 쉽게 구현할 수 있도록 설정 되어 있음.  
간단한 딥러닝 모델을 설계하고 방정식 내에 존재하는 파라미터 업데이트

> **autograd code cell (1)**

```python
import torch

# 파이썬 실행환경에 맞게끔 장비를 설정
if torch.cuda.is_available():
    DEVICE = torch.device('cuda')
else:
    DEVICE = torch.device('cpu')

BATCH_SIZE = 64      # 딥러닝 모델에서 파라미터를 업데이트할 때 계산되는 데이터의 개수
INPUT_SIZE = 1000    # 딥러닝 모델의 Input 크기이자 입력층의 노드 수
HIDDEN_SIZE = 100    # Input을 다수의 파라미터를 이용해 계산한 결과에 한 번 더 계산되는 파라미터 수
OUTPUT_SIZE = 10     # 딥러닝 모델에서 최종으로 출력되는 값의 벡터 크기
```

**4종 파라미터에 대한 구체적인 내용**


1. **`BATCH_SIZE`**
   - 딥러닝 모델에서 파라미터를 업데이트할 때 계산되는 데이터 개수
   - `BATCH_SIZE`의 수 만큼 데이터를 이용해 Output 계산
   - `BATCH_SIZE`의 수 만큼 출력된 결과값에 대한 오차값을 계산
   - `BATCH_SIZE`의 수 만큼 계산된 오차값을 평균 내어 Back Propagation을 적용하고 이를 바탕으로 파라미터를 업데이트
   - 예제에서는 Input으로 이용되는 데이터가 64개  

2. **`INPUT_SIZE`**
   - 딥러닝 모델에서의 Input의 크기이자 입력층의 노드 수
   - 단순히 입력 데이터의 수
   - 예제에서는 입력층 노드 수가 1000개
   - `BATCH_SIZE`와 연계된 해석은 1000 크기의 벡터 값을 64개 이용한다는 뜻
   - 이를 shape으로 설명하자면 (64, 1000)  

3. **`HIDDEN_SIZE`**
   - 딥러닝 모델에서 Input을 다수의 파라미터를 이용해 계산한 결과에 한 번 더 계산되는 파라미터 수
   - 입력층에서 은닉층으로 전달됐을 때 은닉층의 노드 수
   - 예제 기준으로 (64, 1000)의 Input들이 (1000,100) 의 행렬과 행렬곱을 계산  

4. **`OUTPUT_SIZE`**
   - 딥러닝 모델에서 최종으로 출력되는 값의 벡터의 크기
   - 보통 Output의 크기는 비교하고자 하는 레이블의 크기와 동일하게 설정
   - 예를 들어 10개의 레이블로 분류하려면 10짜리 One-Hot Encoding을 이용하기 때문에 `OUTPUT_SIZE`를 10으로 맞추기도 하고, 5 크기의 벡터 값에 대한 MSE를 계산하기 위해 5로 맞추기도 함

> **autograd code cell (2)**

```python
x = torch.randn(BATCH_SIZE,
                INPUT_SIZE,
                device=DEVICE,
                dtype=torch.float,
                requires_grad=False)
y = torch.randn(BATCH_SIZE,
                OUTPUT_SIZE,
                device=DEVICE,
                dtype=torch.float,
                requires_grad=False)
w1 = torch.randn(INPUT_SIZE,
                 HIDDEN_SIZE,
                 device=DEVICE,
                 dtype=torch.float,
                 requires_grad=True)
w2 = torch.randn(HIDDEN_SIZE,
                 OUTPUT_SIZE,
                 device=DEVICE,
                 dtype=torch.float,
                 requires_grad=True)
```
<br><br>

**Input/Output 설정**

---


- `torch.randn` : 평균이 0, 표준편차가 1인 정규분포에서 샘플링한 값 (데이터를 만들어 내는 의미)

- `x` : Input을 설정
   ```text
   BATCH_SIZE(64), INPUT_SIZE(1000) 
      -> 크기 1000짜리의 벡터를 64개 만듦, x는 (64, 1000) 모양의 데이터가 생성
   device = DEVICE
      -> 미리 설정한 DEVICE (cuda or cpu)
   requires_grad = False
      -> Input Data이므로 Gradient를 계산할 필요 없음
      -> Gradient는 파라미터를 업데이트 하기위해 계산하는 것이기 때문
   ```

- `y` : Output을 설정
    ```text
    BATCH_SIZE(64), OUTPUT_SIZE(10)
        -> 크기 10짜리의 벡터를 64개 만듦, y는 (64, 10) 모양의 데이터가 생성
    ((Input 설정과 동일))
    ```
<br><br>

**파라미터 설정**

---

- `w1` : 업데이트 할 파라미터
   ```text
   INPUT_SIZE(1000), HIDDEN_SIZE(100)
      -> INPUT 데이터와 행렬곱을 수행하여 HIDDEN의 개수로 결과 값 산출
   requires_grad = True
      -> Gradient 계산이 들어가므로 True 설정
   ((Input/Output 설정과 동일))
   ```
- `w2` : 업데이트 할 파라미터
   ```text
   HIDDEN_SIZE(100), OUTPUT_SIZE(10)
      -> 최종 라벨 결과가 10개이고 HIDDEN LAYER 이후 가산되므로 (100, 10)
      -> x와 w1의 결과 (64, 1000)*(1000, 100) = (64, 100)
      -> w2의 계산 결과 (64, 100)*(100, 10) = (64, 10)
   ((w1 설정과 동일))
   ```

> **autograd code cell (3)**

```python
learning_rate = 1e-6
for t in range(1, 501):
    y_pred = x.mm(w1).clamp(min=0).mm(w2)
    
    loss = (y_pred-y).pow(2).sum()
    if t % 100 == 0:
        print("Iteration: ", t, "\t", "Loss: ", loss.item())
    loss.backward()
    
    with torch.no_grad():
        w1 -= learning_rate * w1.grad
        w2 -= learning_rate * w2.grad
        w1.grad.zero_()
        w2.grad.zero_()
```
```text
--- out put ---
Iteration:  100 	 Loss:  362.14410400390625
Iteration:  200 	 Loss:  8.020562171936035
Iteration:  300 	 Loss:  10.742217063903809
Iteration:  400 	 Loss:  7.843517303466797
Iteration:  500 	 Loss:  13.475057601928711
```
<br><br>

**code line 해석**

---


```python
learning_rate = 1e-6
```
- 파라미터 업데이트 시, Gradient를 계산한 결과값에 1보다 작은 값을 곱해 업데이트
- 경사 하강에 변화를 주는 수치로 이해하면 될 듯
- 딥러닝 모델에서 중요한 하이퍼 파라미터

---
```python
y_pred = x.mm(w1).clamp(min=0).mm(w2)
```
- `y_pred`는 결과값이고, Input인 x와 중간층 w1의 행렬곱에 clamp 활성화 함수를 적용하고 해당 결과와 w2의 행렬곱을 수행
- `.clamp(min=0)`

    $$
    y_i=
        \begin{cases}
        \min & \mathrm{if}\;x_i<\min \\
        x_i, & \mathrm{if}\;\min \leq x_i \leq \max \\
        \max & \mathrm{if}\;x_i>\max 
        \end{cases}
    $$
    - 최소값이 0으로 설정되었기 때문에 ReLU()와 같은 역할을 함

---
```python
loss = (y_pred-y).pow(2).sum()
```
- 예측값과 실제값의 오차를 계산
- `pow(k)`는 k제곱을 취하는 함수로 제곱 오차의 합을 loss로 설정

---
```python
loss.backward()
```
- 계산된 Loss 값에 대해 `backward()` 메서드를 이용하여 각 파라미터 값에 대한 Gradient를 계산하고 이를 통해 Back Propagation을 진행
- PyTorch 내에 구현된 Back Propagation

---
```python
with torch.no_grad():
```
- 각 파라미터 값에 대해 Gradient를 계산한 결과를 이용하여 파라미터 값을 업데이트할 때는 해당 시점의 Gradient 값을 고정한 후 업데이트 진행
- 코드가 실행되는 시점의 Gradient값을 고정한다는 의미

---
```python
w1 -= learning_rate * w1.grad
w2 -= learning_rate * w2.grad
```
- `w1의 Gradient`와 `learning_rate`를 곱한 값을 기존 `w1`에서 감산
- 음수를 이용하는 이유는 Loss값이 최소로 계산될 수 있는 파라미터 값을 찾기 위해 Gradient 값에 대한 반대 방향으로 계산
- 'w2'도 동일하게 Gradient Decent

---
```python
w1.grad.zero_()
w2.grad.zero_()
```
- 앞 과정에서 각 파라미터 값을 업데이트했다면 각 파라미터 값을 Gradient를 초기화하여 다음 loop를 수행할 수 있도록 Gradient 값을 0으로 설정
- 다음 Backpropagation을 진행할 때 Gradient값을 `loss.backward()`로 새로 계산하기 때문
<br><br>

## 참조

---

- 공부 중인 책 정보 : [파이썬 딥러닝 파이토치](https://github.com/chaaaning/study_DA/blob/main/Deep_Learning/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EB%94%A5%EB%9F%AC%EB%8B%9D%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98.md)
- 해당 내용의 `.ipynb` : [Part1-4.반드시 알아야하는 파이토치 스킬](https://github.com/chaaaning/study_DA/blob/main/Deep_Learning/%ED%8C%8C%EC%9D%B4%EC%8D%AC%EB%94%A5%EB%9F%AC%EB%8B%9D%ED%8C%8C%EC%9D%B4%ED%86%A0%EC%B9%98/P1_4(tensor%2Cautograd).ipynb)

{% if page.comments %}
<div id="post-disqus" class="container">
{% include disqus.html %}
</div>
{% endif %}